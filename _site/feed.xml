<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-07-05T22:17:41-04:00</updated><id>http://localhost:4000/</id><title type="html">Ian Stewart</title><subtitle>Language Geek. Data Freak.</subtitle><entry><title type="html">Reading notes</title><link href="http://localhost:4000/reading_notes/" rel="alternate" type="text/html" title="Reading notes" /><published>2017-06-27T00:00:00-04:00</published><updated>2017-06-27T00:00:00-04:00</updated><id>http://localhost:4000/reading_notes</id><content type="html" xml:base="http://localhost:4000/reading_notes/">&lt;p&gt;This summer, I’ve been doing some reading on social science research methods. That topic is obviously very broad, but I wanted to get a better sense of “how to think like a social scientist” rather than “how to think like a sociolinguist.” I’ve been doing sociolinguistics for so long (6 years?!) that I’ve sort of gotten stuck in a bubble about how to study social phenomena, and the books that I’ve read so far have done a nice job at providing a birds-eye view of quantitative social science methods that I’ve been missing. I’m hosting the notes &lt;a href=&quot;https://github.com/ianbstewart/social_science_methods_readings&quot;&gt;here&lt;/a&gt; if you’re interested!&lt;/p&gt;

&lt;p&gt;So far I’ve covered two short books.&lt;/p&gt;

&lt;h2 id=&quot;designing-social-inquiry-scientific-research-in-qualitative-research-king-keohane-verba-1994-notes-here&quot;&gt;&lt;a href=&quot;http://metodos-avanzados.sociales.uba.ar/files/2014/06/Gary_King_Robert_O._Keohane_Sidney_Verba.pdf&quot;&gt;Designing social inquiry: scientific research in qualitative research&lt;/a&gt; (King, Keohane, Verba 1994) (notes &lt;a href=&quot;https://github.com/ianbstewart/social_science_methods_readings/blob/master/King_1994/King_notes.pdf&quot;&gt;here&lt;/a&gt;)&lt;/h2&gt;

&lt;p&gt;Ever wondered what constitutes “social science research”? Me too! Turns out that there’s a lot of different kinds of social science with varying levels of quantitative and qualitative components, but all social science researchers should be aware of potential pitfalls and biases that might impact how they study social phenomena. King, Keohane and Verba focus mainly on political science and the use of causal inference as a research method to test relationships between dependent and independent variables. Clearly written and strongly argued, this book showed me that there’s no shortcuts to doing interesting research and that even with limited data a researcher can still draw interesting theoretical conclusions, as long as the logic behind the testing is sound.&lt;/p&gt;

&lt;h2 id=&quot;bit-by-bit-social-research-in-the-digital-age-salganik-2017-notes-here&quot;&gt;&lt;a href=&quot;http://www.bitbybitbook.com/&quot;&gt;Bit by bit: social research in the digital age&lt;/a&gt; (Salganik 2017) (notes &lt;a href=&quot;https://github.com/ianbstewart/social_science_methods_readings/blob/master/Salganik_2017/Salganik_notes.pdf&quot;&gt;here&lt;/a&gt;)&lt;/h2&gt;

&lt;p&gt;The digital era is upon us, and we as researchers should adapt to make the most of the new technology and data available. Drawing on a wealth of studies from the past decade, Salganik lays out methodological suggestions for social scientists and data scientists alike to run effective studies in the digital space. Starting from the basics of what constitutes “big data,” Salganik gives examples of different research paradigms such as observational, experimental, and survey studies, as well as the benefits and limitations of each paradigm. I found the section on approximating experiments to be particularly enlightening, as it describes both natural experiments and matching as a way to use observational data to estimate a causal effect in roughly the same way as a typical A/B test. The book also provides a lively discussion of research ethics, which are now more important than ever considering how much personal data is made available online. The digital era has opened up a wealth of opportunities for researchers, including unprecedented levels of mass scientific collaboration, and we should integrate the best of social science theory and data science practice into new studies.&lt;/p&gt;

&lt;p&gt;It’s often too easy to find a social media dataset, then run a boatload of statistical tests and machine learning algorithms on the data without carefully building up an experimental framework and enumerating the assumptions being made, which is how real science works. As a sociolinguist, I hope I can carry the lessons from these books into my future research, even if that means I have to flip how I test a hypothesis or run an experiment.&lt;/p&gt;</content><author><name></name></author><summary type="html">This summer, I’ve been doing some reading on social science research methods. That topic is obviously very broad, but I wanted to get a better sense of “how to think like a social scientist” rather than “how to think like a sociolinguist.” I’ve been doing sociolinguistics for so long (6 years?!) that I’ve sort of gotten stuck in a bubble about how to study social phenomena, and the books that I’ve read so far have done a nice job at providing a birds-eye view of quantitative social science methods that I’ve been missing. I’m hosting the notes here if you’re interested!</summary></entry><entry><title type="html">What does “context” mean? Part two</title><link href="http://localhost:4000/context2/" rel="alternate" type="text/html" title="What does &quot;context&quot; mean? Part two" /><published>2017-06-11T00:00:00-04:00</published><updated>2017-06-11T00:00:00-04:00</updated><id>http://localhost:4000/context2</id><content type="html" xml:base="http://localhost:4000/context2/">&lt;p&gt;As I mentioned last time, my current work is concerned with how linguistic and social context influence the likelihood of a new word’s adoption. Last time I talked about semantic context as the popularity of a word’s “nearest neighbors” and how that might play a role in word adoption.&lt;/p&gt;

&lt;p&gt;The short answer to that question is that the “nearest neighbor” definition of context doesn’t play as big a role as you might think. For instance, if you use neighbor popularity to predict whether a word will grow or decline over time, the accuracy isn’t high enough to justify its use as a predictor. Using neighbor popularity on its own without respect to time, we find that it can differentiate different kinds of words, such as respellings like “ur” which has significantly more popular neighbor “your.” As a side finding, I also figured out that neighbor distance (in vector space) is a pretty bad predictor of polysemy which contradicts &lt;a href=&quot;https://arxiv.org/pdf/1605.09096.pdf&quot;&gt;some previous work&lt;/a&gt;. But neighbor popularity does not seem to influence the likelihood of a word’s adoption, which to me seems like an interesting null result but not enough to build any hypotheses from.&lt;/p&gt;

&lt;p&gt;So what’s another way to think about context? Without resorting to machine learning, we can make context even simpler and consider a word’s context to be the unique number of fixed-word phrases in which it occurs. In the sentence “this is cool af !”, the word “af” occurs in 2 unique bigram contexts: [“cool” “af”] and [“af” “!”]. This seems like a foolproof measure but it turns out to be highly correlated with frequency, i.e. words that have a high frequency also tend to occur in a high number of contexts, as you can see in the plot below (thank you, &lt;a href=&quot;https://en.wikipedia.org/wiki/Heaps%27_law&quot;&gt;Heap’s Law&lt;/a&gt;).&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../docs/blog_images/frequency_unique_contexts.png&quot; width=&quot;500px&quot; /&gt;&lt;/center&gt;

&lt;p&gt;How do we find the words with an especially high or low number of contexts? One way to get at this extra info is to establish an “expected” number of contexts and then compute the difference between the expected and actual number of contexts, i.e. the residual. If we fit a function between the independent variable (frequency) and dependent variable (unique contexts), then we can use the residual between the function’s predicted value and the actual dependent variable. This can help separate the words that occur in an unusually high number of contexts, such as “aka”, from a word with similar frequency that occurs in an unusually low number of contexts, such as “yikes.” You can see this in the plot below.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../docs/blog_images/frequency_context_diversity.png&quot; width=&quot;500px&quot; /&gt;&lt;/center&gt;

&lt;p&gt;Cool! Does this measure shed any light on different word categories? Take different parts of speech, for example. An adverb like “af” would have a lower range of contexts (e.g. only post-adjective) compared with an exclamation such as “yikes” that has fewer syntactic constraints. After tagging the corpus for POS tags using &lt;a href=&quot;https://github.com/brendano/ark-tweet-nlp&quot;&gt;this handy program&lt;/a&gt;, we found that there was a considerable degree of variation in context diversity across part of speech tags. Compare, for example, the higher context diversity of exclamations (marked “!”) compared to the adverbs (marked “R”) in the plot below, where C2 indicates bigram context diversity and C3 indicates trigram context diversity.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../docs/blog_images/pos_context_diversity_distribution.png&quot; width=&quot;700px&quot; /&gt;&lt;/center&gt;

&lt;p&gt;More importantly to our study: does context diversity help us determine whether a lexical innovation will succeed or not? The short answer is more often than not, yes. After picking out a set of lexical innovations that grew and innovations that declined in frequency over the course of the observed timespan (2013-2016), we found that context diversity was the most consistently strong factor in separating the growth words from the decline words, after matching them on frequency. You can see the separation in the plot below, where the successful (growing) innovations “fking,” “af”, and “aka” have a higher initial context diversity than the unsuccessful (declining) innovations “thang”, “protip”, and “heh.”&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../docs/blog_images/frequency_context_diversity_innovation_examples.png&quot; width=&quot;700px&quot; /&gt;&lt;/center&gt;

&lt;p&gt;Why might this be the case? One hypothesis is that innovations that are “on the way in” (such as “af”) are more lexically flexible than innovations that have already seen their prime and are on the way out (e.g. “protip”), which may result in being used in certain fixed expressions rather than flexible expressions (i.e. fewer contexts). This falls in line with Metcalf’s FUDGE model (Metcalf 2004) that proposes a “generative” quality for innovations that allows them to extend to a wide range of contexts.&lt;/p&gt;

&lt;p&gt;The ovearching goal of this project is to compare linguistic context with social context with respect to the adoption of lexical innovations. Our preliminary tests have shown that linguistic context diversity, as measured with the residual-calculation method, is a significantly stronger predictor of success than social context. To me, this suggests that studies that examine innovations but only look at social context are missing out on some important factors. It’s particularly interesting in light of the current focus on social networks as a explanatory mechanism for the adoption of trends like linguistic innovations (e.g. &lt;a href=&quot;https://arxiv.org/pdf/1609.02075.pdf&quot;&gt;this study&lt;/a&gt; by my friend Sandeep).&lt;/p&gt;

&lt;p&gt;Shouldn’t innovation studies, like social network diffusion research, also consider linguistic form and context in addition to social factors? And thinking more broadly, can we abstract this idea to apply to non-linguistic innovations? For example, how do we quantify the diversity of contexts to which a meme applies (quantifying the idea of &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/21Rintel-2013-CrisisMemes-AJPC.pdf&quot;&gt;templatability&lt;/a&gt;)? Not sure there’s an easy answer, but something worth pondering.&lt;/p&gt;

&lt;p&gt;In so many words, that’s the second definition of linguistic context: the relative number of unique bigram and trigram contexts in which a word appears. This definition might seem basic and not terribly sophisticated, but it points to some unexpected trends. If you can find a study that uses this metric or a similar idea, let me know! I haven’t been able to find anything outside of passing mentions in corpus linguistics and machine translation, which aren’t especially helpful.&lt;/p&gt;

&lt;p&gt;We’re currently writing up these results into a paper submission, so hopefully this publication will be out soon (coming to an open-access journal near you)!&lt;/p&gt;</content><author><name></name></author><summary type="html">As I mentioned last time, my current work is concerned with how linguistic and social context influence the likelihood of a new word’s adoption. Last time I talked about semantic context as the popularity of a word’s “nearest neighbors” and how that might play a role in word adoption.</summary></entry><entry><title type="html">What does “context” mean? Part one</title><link href="http://localhost:4000/context1/" rel="alternate" type="text/html" title="What does &quot;context&quot; mean? Part one" /><published>2017-06-07T00:00:00-04:00</published><updated>2017-06-07T00:00:00-04:00</updated><id>http://localhost:4000/context1</id><content type="html" xml:base="http://localhost:4000/context1/">&lt;p&gt;My current research is concerned with the relationship between the social and semantic context of lexical innovations and their likelihood of adoption in the online community Reddit. The innovation &lt;a href=&quot;http://www.urbandictionary.com/define.php?term=Fleek&amp;amp;defid=8099629&quot;&gt;“fleek”&lt;/a&gt; gained success due to its restricted context, i.e. the phrase &lt;a href=&quot;https://www.youtube.com/watch?v=XrTVMXxop3o&quot;&gt;“on fleek”&lt;/a&gt;, but this might be a rarity compared to most innovations that might gain success as a result of being used in a wide variety of contexts. Unlike “fleek,” the intensifier &lt;a href=&quot;http://www.urbandictionary.com/define.php?term=AF&quot;&gt;“af”&lt;/a&gt; (“as fuck”) seems to occur in a wide range of post-adjective contexts (“cool af”, “dope af”, etc.). Related to work on adoption of innovations like &lt;a href=&quot;https://dspace.lboro.ac.uk/dspace-jspui/bitstream/2134/21665/3/Kershaw%20rowe%20stacey%20ACM_proc.pdf&quot;&gt;this&lt;/a&gt; and &lt;a href=&quot;http://publications.aston.ac.uk/28001/1/ELL_078_Grieve_Nini_Guo_final.pdf&quot;&gt;this&lt;/a&gt;, it seems like there is a nontrivial relationship between the linguistic context of a new word and the likelihood of that word being adopted by a community. But how do we study that relationship quantitatively? It’s not easy to come up with a universal definition of “context” apart from the generic &lt;a href=&quot;http://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520&quot;&gt;“company that you keep”&lt;/a&gt; definition, and this still leaves a lot of room for interpretation.&lt;/p&gt;

&lt;p&gt;One way to operationalize this is to define a word’s “context” as its semantic context, meaning the existence of semantically similar words. The word “af” may occur in similar situations as standard intensifiers “very” and “really,” and therefore it may have to “compete” with these standard words in order to gain acceptance. This is a hypothesis that has typically been hard to test because it requires manually defining all the competing words as in the &lt;a href=&quot;https://www.researchgate.net/profile/Alexandra_Darcy/publication/231871295_Frequency_and_variation_in_the_community_grammar_Tracking_a_new_change_through_the_generations/links/55b2421d08aec0e5f43167fd.pdf&quot;&gt;case of quotatives&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One way to get around this issue is with a natural language processing technique known as word embeddings, which allow us to learn about lexical competitors directly from our data: e.g. determining automatically that “af” is competing with “very” and “really.” At the high level, a word embedding model tries to capture lexical semantics by projecting all the words in a corpus into the same high-dimensional vector space and then minimizing the distance between vectors of words that occur in similar contexts - in our example, minimizing the distance between intensifiers “af” and “very” based on their occurrence near adjectives. I give an example in the plot below where I’ve trained a model using Reddit data, then visualized “af” and its nearest “neighbors” in the same 2-D space and sized them according to their frequency. Note that the nearest neighbors include “hella”, “suuuper”, and “extremly” which occur in a similar adverb-ish neighborhood as “af.”&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../docs/blog_images/unstruc_skip_af.png&quot; alt=&quot;Unstructured Skipgram Example&quot; style=&quot;width: 250px;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;One of the many interesting aspects of word embeddings, and NLP in general, is how different models account for different aspects of semantic relationships. For example, the “typical” &lt;a href=&quot;http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;word2vec&lt;/code&gt; model&lt;/a&gt; that I used above defines a word’s context as an unordered “bag” of context words. For instance, in the phrases “that shirt is cool af !” and “that shirt is really cool !”, both “af” and “really” have similar 2-word context “bags” {“is” “cool” “!”} and {“shirt” “is” “cool” “!”}. However, some models like the &lt;a href=&quot;http://www.cs.cmu.edu/~lingwang/papers/naacl2015.pdf&quot;&gt;structured skip-gram&lt;/a&gt; try to capture more syntactic word patterns by treating context as an ordered list of words rather than an unordered “bag” of words. For example, in the phrases “that shirt is cool af !”, “that shirt is cool asf !”, and “that shirt is really cool !” both “af” and “asf” have the same 1-word context [“cool” “!”] but do not have the same context as “really”, [“is” “cool”]. This might seem trivial but it actually changes how the model learns about semantic “similarity”. In the first embedding model “af” may be closer to standard intensifiers like “really” and “very” because they all occur in the same unordered “neighborhood” as adjectives, but in the second model “af” may be closer to post-adjective words like “asf” or “ass” (as in “a cool ass car”) because of how the second model treats “context” as strictly related to word order. You can see this for yourself in the plot below where I’ve visualized “af” and its nearest neighbors based on the structured model.&lt;sup&gt;&lt;a href=&quot;#footnote1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../docs/blog_images/struc_skip_af.png&quot; alt=&quot;Structured Skipgram Example&quot; style=&quot;width: 250px;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;I don’t want to say that one embedding model is inherently better than another but rather that some models treat lexical “semantics” as more of a syntactic rather than purely contextual problem. There’s even &lt;a href=&quot;https://arxiv.org/pdf/1503.00185.pdf&quot;&gt;wackier models&lt;/a&gt; that actually use syntactic parse data to build embeddings, but the nature of the data (Reddit) makes parses unreliable at best and comical at worst so I decided to stick with the “normal” model and the structured model.&lt;/p&gt;

&lt;p&gt;At the end of the day, where does all this machine learning get us? In the case of the syntactically-aware model, it gives us a method to generate likely word competitors based on their shared semantics and syntax in a way that avoids needing domain knowledge and subjective judgments. By narrowing our idea of “context” to “words that occur in similar situations,” we have operationalized our research question in a way that lets us leverage natural language processing. So we can now start to ask questions about how a word’s context influences its likelihood of adoption: when we see a new word like “af”, how much should we bet that its semantic context will affect its likelihood of adoption?&lt;/p&gt;

&lt;p&gt;I’ll end this post with one of the favorite graphs I’ve generated so far, which shows how the up-and-coming “shitposter” has eclipsed one of its neighbors “karmawhore” (determined from the structured embeddings) from 2013 to 2016.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../docs/blog_images/shitposter_competition.png&quot; alt=&quot;Competition Example&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;Next time, I’ll talk about another definition of “context” which is less fancy but, it turns out, a whole lot more useful for studying word adoption and abandonment.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;footnote1&quot;&gt;1&lt;/a&gt;: You might think that getting here was easy, but there’s a lot of data cleaning and number crunching that I’ve swept under the rug because I wanted to get to the fun stuff.&lt;/p&gt;</content><author><name></name></author><summary type="html">My current research is concerned with the relationship between the social and semantic context of lexical innovations and their likelihood of adoption in the online community Reddit. The innovation “fleek” gained success due to its restricted context, i.e. the phrase “on fleek”, but this might be a rarity compared to most innovations that might gain success as a result of being used in a wide variety of contexts. Unlike “fleek,” the intensifier “af” (“as fuck”) seems to occur in a wide range of post-adjective contexts (“cool af”, “dope af”, etc.). Related to work on adoption of innovations like this and this, it seems like there is a nontrivial relationship between the linguistic context of a new word and the likelihood of that word being adopted by a community. But how do we study that relationship quantitatively? It’s not easy to come up with a universal definition of “context” apart from the generic “company that you keep” definition, and this still leaves a lot of room for interpretation.</summary></entry><entry><title type="html">ICWSM 2017</title><link href="http://localhost:4000/ICWSM/" rel="alternate" type="text/html" title="ICWSM 2017" /><published>2017-05-20T00:00:00-04:00</published><updated>2017-05-20T00:00:00-04:00</updated><id>http://localhost:4000/ICWSM</id><content type="html" xml:base="http://localhost:4000/ICWSM/">&lt;p&gt;From May 14-18, I recently attended the &lt;a href=&quot;http://www.icwsm.org/2017/&quot;&gt;International Conference for Web and Social Media&lt;/a&gt; in Montreal to present my work on semantic change that I conducted during my internship at the &lt;a href=&quot;http://www.pnnl.gov/&quot;&gt;Pacific Northwest National Laboratory&lt;/a&gt; during summer 2016. Check out the full paper &lt;a href=&quot;https://arxiv.org/abs/1703.07012&quot;&gt;here&lt;/a&gt; and the poster &lt;a href=&quot;http://ianbstewart.github.io/docs/ICWSM_2017_poster.pdf&quot;&gt;here&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../docs/blog_images/ICWSM_2017_poster.jpg&quot; alt=&quot;Poster&quot; style=&quot;width: 300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since it was my first large-scale conference of grad school, I tried to take in as much as possible. I particularly enjoyed the workshop on &lt;a href=&quot;https://www.microsoft.com/en-us/research/event/ossm17/&quot;&gt;Observational Studies Through Social Media&lt;/a&gt;, which was a nice introduction to causal inference methods like matching. The gist of the methodology is to approximate a randomized experiment using observational data through methods that reduce the potential for confounds affecting the treatment and outcome. Propensity score matching is a nice example of causal inference as it matches “treatment” with “control” users based on similar attributes (example below). It helps researchers tackle tricky questions such as the &lt;a href=&quot;http://www.munmund.net/pubs/icwsm17_SocialSupport.pdf&quot;&gt;influence of social support on suicide ideation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.fico.com/en/sites/default/files/figure4-partial%20overlap.png&quot; alt=&quot;matching example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I tried to take as many notes as possible on the talks and panels, and I’m hosting the notes &lt;a href=&quot;https://docs.google.com/document/d/1Gz0ObQI4oEywrymiwjl1DHmmhW4eqodE0l0ULdzcP40/edit?usp=sharing&quot;&gt;here&lt;/a&gt;. Check them out if you’re interested! There weren’t as many language-centric studies as I had hoped but I still enjoyed the variety of topics covered, from &lt;a href=&quot;https://aiwei.me/files/icwsm2017-ai.pdf&quot;&gt;emoji semantics&lt;/a&gt; to &lt;a href=&quot;https://web.stanford.edu/~jurafsky/pubs/paper-identity.pdf&quot;&gt;community identity&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Overall, I most enjoyed exchanging wacky research ideas with other students and faculty, particularly at informal events like the &lt;a href=&quot;https://sites.google.com/site/icwsmscienceslam/&quot;&gt;Science Slam&lt;/a&gt; where I tried to have fun with semantic change (slides &lt;a href=&quot;https://docs.google.com/presentation/d/1DnEuBA_H3FSw8kaOWeEcVbtTblmEjtS0o_u7AO6nQv0/edit?usp=sharing&quot;&gt;here&lt;/a&gt;). The winning talk was on breakups on Twitter - messy affairs. I wish all conferences had informal academic exchanges like the Science Slam - it’s a nice reminder of how fun the field of social computing can be.&lt;/p&gt;

&lt;p&gt;I also had a tiny bit of free time to celebrate Montreal’s 375th birthday - no idea that it was older than America! The Jacques Cartier Bridge was all lit up and fireworks were everywhere. I hope to do some more exploring next time I visit Montreal!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://scontent-iad3-1.xx.fbcdn.net/v/t1.0-9/18519819_10208969117765281_7150643777878557114_n.jpg?oh=61bb3a5090c3d27631ae971ac40d99af&amp;amp;oe=59B77C9B&quot; alt=&quot;Montreal celebration&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">From May 14-18, I recently attended the International Conference for Web and Social Media in Montreal to present my work on semantic change that I conducted during my internship at the Pacific Northwest National Laboratory during summer 2016. Check out the full paper here and the poster here!</summary></entry></feed>